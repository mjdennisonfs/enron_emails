{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6cf7ea4",
   "metadata": {},
   "source": [
    "# Enron E-mail dataset\n",
    "\n",
    "This notebook anlyses the Enron email dataset, downloaded from: <br>\n",
    "https://www.cs.cmu.edu/~./enron/\n",
    "<br>\n",
    "The dataset contains emails from Enron employees released as part of a US government investigation.\n",
    "<br><br>\n",
    "This notebook covers the following:\n",
    "- Carry out an initial analysis of the dataset.\n",
    "- Clean the dataset and remove any redundant data.\n",
    "- Gain statistical insights from the data.\n",
    "\n",
    "\n",
    "The insights section will focus on:\n",
    "- basic anlysis of the email metadata (who sent what and when)\n",
    "- clustering email text: can we use basic K-means clustering to identify groups of emails \n",
    "    - this can form the basis for labelling data for further anlysis\n",
    "- topic analysis: can we extract topics for different groups of emails\n",
    "    - this can be used to analyse what users are sending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# import external packages\n",
    "import sys\n",
    "cwd = Path(os.getcwd())\n",
    "sys.path.append(str(cwd.parent))\n",
    "from data_utils.data_extraction import (\n",
    "    get_metadata,\n",
    "    get_email_body,\n",
    "    strip_string,\n",
    "    stem_text,\n",
    "    remove_named_entities,\n",
    "    extract_message_nouns\n",
    ")\n",
    "from data_utils.model_fitting import fit_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the dataset (set to correct path for the user)\n",
    "data_path = Path(\"/Users/matthew/tmp/feedstock/maildir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d19b58",
   "metadata": {},
   "source": [
    "## Initial analysis\n",
    "The aim of this section is to understand the dataset (e.g. what it contains, how the data is stored and formatted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d2f6a",
   "metadata": {},
   "source": [
    "### Data storage\n",
    "The dataset contains emails from 150 users. The username is used as a directory in the first layer of data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [os.path.basename(u) for u in glob.glob(str(data_path / \"*\"))]\n",
    "print(f\"Number of users: {len(users)}\")\n",
    "print(\"\")\n",
    "print(\"10 random users:\")\n",
    "print(\"-\" * len(\"10 random users:\"))\n",
    "print(\"\\n\".join(sorted(np.random.choice(users, 10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65aae00",
   "metadata": {},
   "source": [
    "The emails are stored in the following format:<br><br>\n",
    "*{user_name}/{email_folder}/{email_subfolder}/{email_name}*\n",
    "<br><br>\n",
    "- __email_folder__: the main folder the email is stored in, e.g. __inbox__, __sent__ etc.\n",
    "- __email_subfolder__: there may be no sub-folder or more than 1.\n",
    "- __email_name__: this is a number with a full-stop at the end, giving a count of emails in each folder/sub-folder (e.g. 1., 2. 3. etc.)\n",
    "<br><br>\n",
    "\n",
    "Below we store the location of each email in dataframe, using the following columns. Using this, we can read in any email by reconstructing the path to it for a given index value of the dataframe.\n",
    "- __user__: user name of the email sender.\n",
    "- __path__: path to the email from the email sender directory, which will be {email_folder}/{email_subfolder}\n",
    "- __fname__: file name for the email, e.g. 1., 2., 3. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f970eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"user\", \"path\", \"fname\"])\n",
    "\n",
    "for user in users:\n",
    "    # email files end in .\n",
    "    files = glob.glob(str(Path(data_path) / user / \"**\" / \"*.\"), recursive=True)\n",
    "    # find the filename\n",
    "    fnames = [os.path.basename(f) for f in files]\n",
    "    # find the path from data_path\n",
    "    paths = [os.path.dirname(f) for f in files]\n",
    "    paths = [p.split(f\"{data_path}/{user}/\")[-1] if p != f\"{data_path}/{user}\" else \"\" for p in paths]\n",
    "    df_sub = pd.DataFrame(\n",
    "        columns=[\"user\", \"path\", \"fname\"],\n",
    "        data=zip([user] * len(files), paths, fnames)\n",
    "    )\n",
    "    df = pd.concat([df, df_sub], axis=0)\n",
    "df = df.sort_values([\"user\", \"path\", \"fname\"]).reset_index(drop=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518672e",
   "metadata": {},
   "source": [
    "### Overview of email senders\n",
    "Of the 150 users, there are large differences in the number of emails sent/received.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a06d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total emails in dataset: {len(df)}\")\n",
    "print(\"\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Emails per user (absolute values):\")\n",
    "vc = df[\"user\"].value_counts()\n",
    "display(vc.describe())\n",
    "print(f\"IQR:               {round(np.quantile(vc.values, 0.75) - np.quantile(vc.values, 0.25), 2)}\")\n",
    "print(f\"Middle 95% range:  {round(np.quantile(vc.values, 0.975) - np.quantile(vc.values, 0.025), 2)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Emails per user (percentage values):\")\n",
    "vc = df[\"user\"].value_counts() * 100.0 / len(df)\n",
    "display(vc.describe())\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Users responsible for most emails (as a %):\")\n",
    "display(vc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(vc)), vc.values)\n",
    "plt.title(\"Percentage of all emails sent+received by each user\", fontsize=12)\n",
    "plt.ylabel(\"% of emails\", fontsize=12)\n",
    "plt.xlabel(\"User rank\", fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5aeb1c",
   "metadata": {},
   "source": [
    "### Email storage\n",
    "- Most emails are stored in default folders (e.g. all_documents, inbox, sent).\n",
    "- Many folders have a single email in them.\n",
    "- Some non-default folder names have many emails e.g. bill_williams_iii.\n",
    "\n",
    "- all_documents needs to be checked that it does not duplicate other folders.\n",
    "- Some folders can be identified as sent or received, others are not so clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df[\"path\"].value_counts()\n",
    "print(f\"Number of unique folders (including sub-folders): {len(vc)}\")\n",
    "print(\"\")\n",
    "print(\"Number of emails in each folder:\")\n",
    "display(vc.head(20))\n",
    "display(vc.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66be7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential folders for sent emails\n",
    "sent_folders = sorted(\n",
    "    [v for v in vc.index if (\"present\" not in v.lower()) and (\"sent\" in v.lower()) or (\"outb\" in v.lower())]\n",
    ")\n",
    "n_sent = vc[sent_folders].sum()\n",
    "n_rec = vc[~vc.index.isin(sent_folders)].sum()\n",
    "n_tot = vc.sum()\n",
    "print(f\"Number of sent emails:     {n_sent} ({round(100.0 * n_sent / n_tot)}%)\")\n",
    "print(f\"Number or received emails: {n_rec} ({round(100.0 * n_rec / n_tot)}%)\")\n",
    "print(\"\")\n",
    "print(\"\\n\".join(sent_folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616dba6",
   "metadata": {},
   "source": [
    "### Extracting email meta-data\n",
    "The metadata for the emails is stored at the top of the email text. The format is:\n",
    "<br><br>\n",
    "metadata_key_1: metadata_value_1<br>\n",
    "metadata_key_2: metadata_value_2<br>\n",
    "...<br>\n",
    "...<br>\n",
    "metadata_key_N: metadata_value_N<br>\n",
    "<br><br>\n",
    "These are extracted from the email body and stored in a dataframe:\n",
    "- columns: metadata keys\n",
    "- index: email index, as in df\n",
    "- values: the metadata values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe628a",
   "metadata": {},
   "source": [
    "#### Metadate columns\n",
    "\n",
    "The metadata has the following columns (with our data for user, path and fname added).\n",
    "\n",
    "- __Message-ID__: Unique id for the email.\n",
    "- __Mime-Version__: Stands for \"Multipurpose Internet Mail Extension\"\n",
    "- __Date__: Date that the email was sent, with timezone.\n",
    "- __From__: Sender of the email, not necessarily the same\n",
    "- __To__: Receiver of the email.\n",
    "- __Subject__: Subject line of the email.\n",
    "- __Content-Type__: Encoding of the email body text.\n",
    "- __Content-Transfer-Encoding__: Mechanism for re-encoding data for sending the email.\n",
    "- __X-From__: Name of the sender (with email address if external)\n",
    "- __X-To__: Name of the recipient, with email address in some cases (mostly external)\n",
    "- __X-cc__: List of CC'd recipients.\n",
    "- __X-bcc__: List of BCC'd recipients.\n",
    "- __X-Folder__: Storage folder of the email.\n",
    "- __X-Origin__: Origin of the email, not necessarily the same as the user, could be due to errors in storing the emails.\n",
    "- __X-FileName__: Notes Storage Facility filename (used by IBM Notes to store email data).\n",
    "- __Cc__: List of CC'd recipients, sometimes different from X-cc.\n",
    "- __Bcc__: List of BCC'd recipients, seems to be the same as CC.\n",
    "- __user__: Username, from the folder the emails are stored in.\n",
    "- __path__: Path the email is stored in, from the user folder (e.g. username/path/1.)\n",
    "- __fname__: Filename of the email, stored as 1., 2., 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83e06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in all the metadata, using get_metadata function\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "contents_meta_all = {}\n",
    "\n",
    "# extract metadata for a sample of N emails (use all or a sub-sample)\n",
    "# Note: it can take ~30mins to read in all the metadata (for ~500,000 emails)\n",
    "N = len(df)\n",
    "if N < len(df):\n",
    "    inds = sorted(np.random.choice(range(len(df)), N, replace=False))\n",
    "else:\n",
    "    inds = list(df.index)\n",
    "\n",
    "for i, ind in enumerate(inds):\n",
    "    fname = os.path.join(data_path, \"/\".join(df.loc[ind, [\"user\", \"path\", \"fname\"]].values))\n",
    "    with open(fname, \"r\", errors='replace') as f:\n",
    "        contents = f.readlines()\n",
    "        try:\n",
    "            contents_meta = get_metadata(contents)\n",
    "        except ValueError:\n",
    "            print(f\"{i}: No meta data for {fname}\")\n",
    "            contents_meta = {}\n",
    "        contents_meta_all[ind] = contents_meta\n",
    "    if np.remainder((i + 1), int(N / 100)) == 0:\n",
    "        print(f\"complete: {round(100.0 * (i + 1) / N, 2)}% in {round(time.time() - start_time, 2)}s\")\n",
    "df_meta = pd.DataFrame.from_dict(contents_meta_all, orient=\"index\")\n",
    "print(time.time() - start_time)\n",
    "display(df_meta.sample(n=20))\n",
    "# cleanup\n",
    "del contents_meta_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a72cd",
   "metadata": {},
   "source": [
    "Make sure no rows are duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicated rows: {df_meta.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9c618",
   "metadata": {},
   "source": [
    "We now check for redundant columns. That is, those where all values are the same, or that duplicate other columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for redundant columns\n",
    "\n",
    "# - Message-ID we can drop, each is unique and we use the integer index as a unique ID\n",
    "# - Mime-Version (Multipurpose Internet Mail Extensions) is always the same\n",
    "# - X-bcc has a less than 1% non-empty values\n",
    "print(\"column name\".ljust(30), \"num. unique vals\".ljust(25), \"% unique vals\")\n",
    "for col in df_meta.columns:\n",
    "    print(\n",
    "        col.ljust(30),\n",
    "        str(len(df_meta[col].unique())).ljust(25),\n",
    "        round(100.0 * len(df_meta[col].unique()) / len(df_meta), 4))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Columns with few unique values:\")\n",
    "for col in [col for col in df_meta.columns if len(df_meta[col].unique()) < 10]:\n",
    "    print(\"\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    print(col)\n",
    "    display(df_meta[col].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22df43",
   "metadata": {},
   "source": [
    "#### Missing data\n",
    "Some columns have missing data. In some cases, metadata was not present for an email (Cc, Bcc). In other cases, the key was present, but there is value, and it is stored here as an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb62189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To is missing for some emails\n",
    "# Cc and Bcc have missing values, and seem to be the same overall\n",
    "display(df_meta.loc[:, df_meta.isnull().sum(axis=0) != 0].isnull().sum(axis=0))\n",
    "\n",
    "# Bcc and Cc both missing is the same as when either is missing\n",
    "print(f\"Num where Bcc and Cc are both NaN:    {df_meta[['Cc', 'Bcc']].isnull().all(axis=1).sum()}\")\n",
    "# Bcc and Cc not missing, have the same value 100% of the time\n",
    "inds = df_meta[['Cc', 'Bcc']].notnull().all(axis=1)\n",
    "print(f\"% of entries where Bcc = Cc != NaN:   {100.0 * (df_meta.loc[inds, 'Cc'] == df_meta.loc[inds, 'Bcc']).mean()}%\")\n",
    "\n",
    "# This means that we can drop Bcc, since it is the same as Cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f17bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need\n",
    "# - Bcc is the same as Cc\n",
    "# - Message-ID is not needed, we use the dataframe index as the id\n",
    "# - Mime-Version is the same for all emails\n",
    "df_meta = df_meta.drop([\"Bcc\", \"Message-ID\", \"Mime-Version\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc23480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the user/path info\n",
    "df_meta = df_meta.merge(df, left_index=True, right_index=True, how=\"left\")\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dea81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data (set to True)\n",
    "if False:\n",
    "    df_meta.to_csv(str(data_path.parent / \"meta.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ef228",
   "metadata": {},
   "source": [
    "### Email time / date\n",
    "The time and date that the email was sent, including the timezone, is stored in the metadata.\n",
    "<br>\n",
    "No values are missing. We can parse these, and explore when emails are sent/received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timezones (probably just tell us time of year)\n",
    "df_meta[\"Date\"].str.replace(r'[^(]*\\(|\\)[^)]*', '').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a datetime, using UTC (since the dates are timezone aware)\n",
    "df_meta[\"datetime_utc\"] = pd.to_datetime(df_meta[\"Date\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6682f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First look at the year:\n",
    "\n",
    "# Most data is from 2000 - 2001\n",
    "# 1999 and 2002 also have data\n",
    "\n",
    "# Some are clearly incorrect (e.g. 1980, 2043)\n",
    "\n",
    "print(\"value counts for years\")\n",
    "display(df_meta[\"datetime_utc\"].dt.year.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efcbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1997 and 1998 could be actual emails\n",
    "\n",
    "# 1997: most emails are from 1 user \n",
    "display(df_meta.loc[df_meta[\"datetime_utc\"].dt.year == 1997, \"user\"].value_counts())\n",
    "# 1998: most emails are from 1 user \n",
    "display(df_meta.loc[df_meta[\"datetime_utc\"].dt.year == 1998, \"user\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2f0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some have a date of 1980: all same time, must be an error / placeholder valaue\n",
    "df_meta[df_meta[\"datetime_utc\"].dt.year == 1980]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e31540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at years that do not occur a lot: these seem to be spam (from none Enron email addresses)\n",
    "# also all are outlier years from the data\n",
    "# probably have data for 1997 to 2002\n",
    "\n",
    "vc = df_meta[\"datetime_utc\"].dt.year.value_counts()\n",
    "for year in vc[vc.values < 100].index:\n",
    "    print(\"\")\n",
    "    print(\"-\" * 50)\n",
    "    print(year)\n",
    "    data = df_meta[df_meta[\"datetime_utc\"].dt.year == year]\n",
    "    display(data[\"From\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d21df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to remove 1980s and the other years where the counts are low for any analysis on years\n",
    "years = list(vc[vc.values < 100].index) + [1980]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70441e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of emails by time\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.hist(df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), \"datetime_utc\"], bins=100)\n",
    "plt.title(\"distribution of number of emails over time\", fontsize=14)\n",
    "plt.xlabel(\"date\", fontsize=14)\n",
    "plt.ylabel(\"count(emails)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83977323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next look at basic date\n",
    "\n",
    "# Large variation, some days with several 1000, some with 1\n",
    "\n",
    "print(\"Value counts for dates:\")\n",
    "vc = df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), \"datetime_utc\"].dt.date.value_counts()\n",
    "display(vc)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.hist(df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), \"datetime_utc\"].dt.date, bins=500, density=1.0)\n",
    "plt.title(\"Distribution of dates\", fontsize=14)\n",
    "plt.xlabel(\"date\", fontsize=14)\n",
    "plt.ylabel(\"P(date)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of the month is fairly constant (less months have 31 days)\n",
    "vc = df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), \"datetime_utc\"].dt.day.value_counts()\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.bar(vc.index, vc.values)\n",
    "plt.title(\"Count of day of month\", fontsize=14)\n",
    "plt.xlabel(\"day of month\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4616f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekends have less emails sent and received\n",
    "# Mid-week has most\n",
    "\n",
    "vc = df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), \"datetime_utc\"].dt.day_name().value_counts()\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.barh(vc.index, vc.values)\n",
    "plt.title(\"Count of day of week\", fontsize=14)\n",
    "plt.ylabel(\"day of week\", fontsize=14)\n",
    "plt.xlabel(\"Count\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summer months produce less emails\n",
    "# Autumn produces the most (catch up after summer?)\n",
    "\n",
    "vc = df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), \"datetime_utc\"].dt.month_name().value_counts()\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.barh(vc.index, vc.values)\n",
    "plt.title(\"Count of month\", fontsize=14)\n",
    "plt.ylabel(\"month\", fontsize=14)\n",
    "plt.xlabel(\"Count\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff61054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# email rates: how many emails each user sends/receives per day\n",
    "\n",
    "tmp = (\n",
    "    df_meta.loc[~df_meta[\"datetime_utc\"].dt.year.isin(years), :]\n",
    "    .groupby(\"user\")[\"datetime_utc\"].agg([\"count\", \"min\", \"max\"])\n",
    ")\n",
    "tmp[\"range\"] = (tmp[\"max\"] - tmp[\"min\"]).dt.days\n",
    "tmp[\"email_rate\"] = tmp[\"count\"] / tmp[\"range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tmp.sort_values([\"email_rate\", \"count\", \"range\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"linder-e\" has the highest email rate: 2805 in 49 days. Some seem to be repeated\n",
    "display(df_meta.loc[df_meta[\"user\"] == \"linder-e\", \"Subject\"].value_counts())\n",
    "print(\"-\" * 80)\n",
    "# \"symes-k\" is similar\n",
    "display(df_meta.loc[df_meta[\"user\"] == \"symes-k\", \"Subject\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also look at:\n",
    "# words per day\n",
    "# chars per day\n",
    "# corr between words per email and num emails\n",
    "# corr between words per email and avg. num recipients (to + cc + bcc, as a set)\n",
    "display(tmp[\"email_rate\"].agg([\"min\", \"median\", \"max\", \"mean\", \"std\"]))\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.hist(tmp[\"email_rate\"], bins=100)\n",
    "plt.title(\"distribution of email rates (emails per day)\", fontsize=14)\n",
    "plt.xlabel(\"email rate\", fontsize=14)\n",
    "plt.ylabel(\"count(email rate)\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69602ff7",
   "metadata": {},
   "source": [
    "### From and To\n",
    "The From and To metadata keys show who sent and who recveived an email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427f58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sometimes From == TO\n",
    "tmp = df_meta[(df_meta[\"From\"] == df_meta[\"To\"])]\n",
    "print(f\"Number where From == To: {len(tmp)}, {round(100.0 * len(tmp) / len(df_meta), 2)}%\")\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b718e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# often, but not always, these have CC's\n",
    "vc = tmp[\"X-cc\"].value_counts()\n",
    "# trim the Ccs to 80 chars, easier to view\n",
    "vc.index = [i[:80] for i in vc.index]\n",
    "display(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Tos are missing\n",
    "inds = df_meta[\"To\"].isnull()\n",
    "print(f\"Number of missing \\\"To\\\" entries:            {inds.sum()}, {round(100.0 * inds.mean(), 2)}%\")\n",
    "# A few have Cc values even with no To values\n",
    "inds = df_meta[[\"To\", \"Cc\"]].isnull().all(axis=1)\n",
    "print(f\"Number of missing \\\"To\\\" and \\\"Cc\\\" entries:   {inds.sum()}, {round(100.0 * inds.mean(), 2)}%\")\n",
    "#display(df_meta.loc[inds, \"Cc\"].str.split(\", \").apply(len).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec0236",
   "metadata": {},
   "source": [
    "### Subject\n",
    "The email subject can be examined in two ways:\n",
    "- length of the subject (e.g. number of words, number of chars)\n",
    "- the actual subject (e.g. what terms are used) \n",
    "<br><br>\n",
    "A full analysis of the data could be used to find correlations between e.g. subject length and the time to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06041bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length in chars and words\n",
    "subject_char_lens = df_meta[\"Subject\"].fillna('').apply(len)\n",
    "subject_word_lens = df_meta[\"Subject\"].fillna('').str.split().apply(len)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(subject_char_lens, bins=100)\n",
    "plt.title(\"Distribution of subject lengths (chars)\", fontsize=12)\n",
    "plt.xlabel(\"len\", fontsize=12)\n",
    "plt.ylabel(\"P(len)\", fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(subject_word_lens, bins=100)\n",
    "plt.title(\"Distribution of subject lengths (words)\", fontsize=12)\n",
    "plt.xlabel(\"len\", fontsize=12)\n",
    "plt.ylabel(\"P(len)\", fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: User, X-Origin and From are not the same thing\n",
    "# Below shows, for each user, the number of unique X-Origin values, with some having 2+\n",
    "df_meta.groupby(\"user\")[\"X-Origin\"].agg([\"count\", \"nunique\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6293884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Unique X-Origin: {len(df_meta['X-Origin'].unique())}\")\n",
    "print(f\"Unique X-From:   {len(df_meta['X-From'].unique())}\")\n",
    "print(f\"Unique From:     {len(df_meta['From'].unique())}\")\n",
    "print(f\"Unique user:     {len(df_meta['user'].unique())}\")\n",
    "\n",
    "# loop over each user, find those with more than 2 X-Origin values\n",
    "# Some appear to be misspellings of the same name, others are mixed up\n",
    "# Would need to check that the data is stored in the correct folder\n",
    "\n",
    "for t in df_meta[\"user\"].unique():\n",
    "    vc = df_meta.loc[df_meta[\"user\"] == t, \"X-Origin\"].str.lower().value_counts()\n",
    "    if len(vc) > 1:\n",
    "        print(\"\\n\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"User: {t}\")\n",
    "        display(vc.to_frame())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dad1b",
   "metadata": {},
   "source": [
    "### Message bodies\n",
    "The body of the message is read in as follows:\n",
    "- take the lines in the email file below the final metadata line\n",
    "- strip any html in the email\n",
    "- merge the lines into a single string\n",
    "- replace all whitespace with a single space\n",
    "<br><br>\n",
    "The messages may contain additional text, e.g. the message they are replying to (Original Message), or the message being forwarded (Forwarded Message). For now, we leave these in the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33834b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "msgs_all = {}\n",
    "\n",
    "# extract message bodies for a sample of N emails (use all or a sub-sample)\n",
    "# Note: it can take ~50mins to read in all the messages (for ~500,000 emails)\n",
    "N = len(df_meta)\n",
    "if N < len(df_meta):\n",
    "    inds = sorted(np.random.choice(range(len(df_meta)), N, replace=False))\n",
    "else:\n",
    "    inds = list(df_meta.index)\n",
    "\n",
    "for i, ind in enumerate(inds):\n",
    "    fname = os.path.join(data_path, \"/\".join(df_meta.loc[ind, [\"user\", \"path\", \"fname\"]].values))\n",
    "    with open(fname, \"r\", errors='replace') as f:\n",
    "        contents = f.readlines()\n",
    "        try:\n",
    "            msg = get_email_body(contents)\n",
    "        except ValueError:\n",
    "            print(f\"{i}: No email body for {fname}\")\n",
    "            msg = \"\"\n",
    "        msgs_all[ind] = msg\n",
    "    if np.remainder((i + 1), int(N / 100)) == 0:\n",
    "        print(f\"complete: {round(100.0 * (i + 1) / N, 2)}% in {round(time.time() - start_time, 2)}s\")\n",
    "df_msgs = pd.DataFrame(msgs_all.values(), index=msgs_all.keys(), columns=[\"msg\"])\n",
    "print(time.time() - start_time)\n",
    "display(df_msgs.sample(n=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # save the messages (set to True to do so)\n",
    "    df_msgs.to_csv(str(data_path.parent / \"msgs.csv\"))\n",
    "if False:\n",
    "    # read in the messages (set to True to do so)\n",
    "    df_msgs = pd.read_csv(str(data_path.parent / \"msgs.csv\"), index_col=0)\n",
    "    df_meta = pd.read_csv(str(data_path.parent / \"meta.csv\"), index_col=0)\n",
    "    print(df_meta.shape, df_msgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a18e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove forwarded and original messages\n",
    "if False:\n",
    "    df_msgs[\"msg\"] = df_msgs[\"msg\"].str.split(\"----- Forwarded\").str[0]\n",
    "    df_msgs[\"msg\"] = df_msgs[\"msg\"].str.split(\"-----Original\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58633465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing messages (only had html)\n",
    "inds_keep = df_msgs[df_msgs[\"msg\"].notnull()].index\n",
    "df_msgs = df_msgs.loc[inds_keep, :]\n",
    "df_meta = df_meta.loc[inds_keep, :]\n",
    "print(df_meta.shape, df_msgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de37dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some messages are duplicated (about 50%)\n",
    "# These can be removed for the following analysis\n",
    "f\"Duplicated messages: {round(100.0 * df_msgs.duplicated().mean(), 2)}%\"\n",
    "df_meta = df_meta.loc[~df_msgs.duplicated(), :]\n",
    "df_msgs = df_msgs.loc[~df_msgs.duplicated(), :]\n",
    "print(df_meta.shape, df_msgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data now read in and stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1700fad7",
   "metadata": {},
   "source": [
    "### Text clustering\n",
    "Below we implement an algorithm to cluster the data:\n",
    "- Split the data into train, validation and test sets\n",
    "- Fit a TF-IDF vectorizer to the train set\n",
    "- Extract TF-IDF feature vectors for the train, val and test sets\n",
    "- Fit a K-Means model to the train set, using the val set to find the optimal number of centers (using the Silhouette Score)\n",
    "<br><br>\n",
    "\n",
    "The Silhouette Score goes from -1 (bad fit) to +1 (good fit) and is the mean Silhouette Coefficient of all samples, which is given by `(b - a) / max(a, b)`, where\n",
    "- a: the mean intra-cluster distance for each sample.\n",
    "- b: the mean nearest-cluster distance for each sample. \n",
    "<br><br>\n",
    "\n",
    "Once this model is fitted, we can use PCA to reduce the feature vectors to 2d and visualize the clusters.\n",
    "<br><br>\n",
    "\n",
    "The aim is to find patterns in the clusters, for e.g.:\n",
    "- can we determine types of email (e.g. personal vs business, spam emails)\n",
    "- can we find an interesting sub-set of emails to examine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, NMF\n",
    "\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c48ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data:\n",
    "# fit to a random sub-sample of the dataset\n",
    "# predict on a different sub-sample\n",
    "inds = list(df_msgs.index)\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "n_train = 50000\n",
    "n_val = 10000\n",
    "n_test = 10000\n",
    "\n",
    "inds_train = inds[:n_train]\n",
    "inds_val = inds[n_train : n_train + n_val]\n",
    "inds_test = inds[n_train + n_val : n_train + n_val + n_test]\n",
    "\n",
    "df_msgs_train = df_msgs.loc[inds_train, :].copy(deep=True)\n",
    "df_msgs_test = df_msgs.loc[inds_test, :].copy(deep=True)\n",
    "df_msgs_val = df_msgs.loc[inds_val, :].copy(deep=True)\n",
    "del df_msgs\n",
    "\n",
    "df_meta_train = df_meta.loc[inds_train, :].copy(deep=True)\n",
    "df_meta_test = df_meta.loc[inds_test, :].copy(deep=True)\n",
    "df_meta_val = df_meta.loc[inds_val, :].copy(deep=True)\n",
    "del df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4ef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2f6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip and stem the messages\n",
    "df_msgs_train[\"msg_strip\"] = strip_string(df_msgs_train[\"msg\"].values.tolist())\n",
    "df_msgs_val[\"msg_strip\"] = strip_string(df_msgs_val[\"msg\"].values.tolist())\n",
    "df_msgs_test[\"msg_strip\"] = strip_string(df_msgs_test[\"msg\"].values.tolist())\n",
    "\n",
    "# note, results are worse with stemming, so leave at this time\n",
    "if False:\n",
    "    df_msgs_train[\"msg_strip\"] = stem_text(df_msgs_train[\"msg_strip\"].values.tolist())\n",
    "    df_msgs_val[\"msg_strip\"] = stem_text(df_msgs_val[\"msg_strip\"].values.tolist())\n",
    "    df_msgs_test[\"msg_strip\"] = stem_text(df_msgs_test[\"msg_strip\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove named entities, replace with their type\n",
    "# not used now due to speed issues.\n",
    "if False:\n",
    "    df_msgs_train[\"msg_strip\"] = remove_named_entities(df_msgs_train[\"msg_strip\"].values.tolist())\n",
    "    df_msgs_val[\"msg_strip\"] = remove_named_entities(df_msgs_val[\"msg_strip\"].values.tolist())\n",
    "    df_msgs_test[\"msg_strip\"] = remove_named_entities(df_msgs_test[\"msg_strip\"].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50692dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vectorizer to all the data, keep 10,000 features only due to memory constraints\n",
    "vec = fit_tf_idf(\n",
    "    msgs=df_msgs_train[\"msg_strip\"].values.tolist(),\n",
    "    max_features=10000,\n",
    "    max_df=0.95,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "# get the features for the sample data\n",
    "features_train = vec.transform(df_msgs_train[\"msg_strip\"].values.tolist())\n",
    "features_val = vec.transform(df_msgs_val[\"msg_strip\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ed59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave the non-alpha terms / names / etc. in for now\n",
    "\n",
    "# we could replace names (e.g. \"glynn\") with a placeholder (e.g. \"PERSON\"), although this\n",
    "# operation is expensive and will be left for now\n",
    "\n",
    "print(np.random.choice(list(vec.vocabulary_.keys()), 100, replace=False))\n",
    "vocab = list(vec.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93147cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through number of clusters, see which has lowest silhouette score\n",
    "\n",
    "for n_clusters in [2, 3, 4, 5, 6, 7, 8]:\n",
    "    cls = MiniBatchKMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cls.fit(features_train)\n",
    "    # predict cluster labels for new dataset\n",
    "    df_msgs_val[f\"label_{n_clusters}\"] = cls.predict(features_val)\n",
    "    # to get cluster labels for the dataset used while\n",
    "    # training the model (used for models that does not\n",
    "    # support prediction on new dataset).\n",
    "    vc = df_msgs_val[f\"label_{n_clusters}\"].value_counts()\n",
    "    # get a sample for each label\n",
    "    sample_size = min(vc.min(), int(8000 / n_clusters))\n",
    "    if sample_size != 1:\n",
    "        inds = []\n",
    "        for i in vc.index:\n",
    "            inds += list(df_msgs_val[df_msgs_val[f\"label_{n_clusters}\"] == i].sample(n=sample_size).index)\n",
    "        inds = df_msgs_val.index.isin(inds)\n",
    "        sil_score = silhouette_score(\n",
    "                features_val[inds],\n",
    "                labels=df_msgs_val[f\"label_{n_clusters}\"].values[inds]\n",
    "            )\n",
    "        print(n_clusters, sil_score, vc.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8690e9f",
   "metadata": {},
   "source": [
    "- 2 and 3 and 7 both have roughly similar scores (we're using a small subsample of the data, so we'd expect large errors in these values).\n",
    "- The analysis will continue with 3 clusters:\n",
    "    - start with a small number, and increase later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cdb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use 3, re-fit to the data\n",
    "n_clusters = 3\n",
    "\n",
    "cls = MiniBatchKMeans(n_clusters=n_clusters, random_state=0)\n",
    "cls.fit(features_train)\n",
    "\n",
    "# predict cluster labels for new dataset\n",
    "features_test = vec.transform(df_msgs_test[\"msg_strip\"].values.tolist())\n",
    "df_msgs_test[\"label\"] = cls.predict(features_test)\n",
    "\n",
    "# to get cluster labels for the dataset used while\n",
    "# training the model (used for models that does not\n",
    "# support prediction on new dataset).\n",
    "vc = df_msgs_test[\"label\"].value_counts()\n",
    "\n",
    "sample_size = min(vc.min(), int(8000 / n_clusters))\n",
    "if sample_size != 1:\n",
    "    inds = []\n",
    "    for i in vc.index:\n",
    "        inds += list(df_msgs_test[df_msgs_test[\"label\"] == i].sample(n=sample_size).index)\n",
    "    inds = df_msgs_test.index.isin(inds)\n",
    "    sil_score = silhouette_score(\n",
    "            features_test[inds],\n",
    "            labels=df_msgs_test[\"label\"].values[inds]\n",
    "        )\n",
    "    print(n_clusters, sil_score, vc.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the features to 2D using PCA\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "# use the incemental PCA for large datasets\n",
    "#pca = IncrementalPCA(n_components=2, batch_size=1000)\n",
    "\n",
    "# fit and transform\n",
    "reduced_features = pca.fit_transform(features_test.toarray())\n",
    "\n",
    "# reduce the cluster centers to 2D\n",
    "reduced_cluster_centers = pca.transform(cls.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee92798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# cluster centers are plotted as black dots\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in df_msgs_test[\"label\"].value_counts().index:\n",
    "    inds = df_msgs_test[\"label\"] == i\n",
    "    plt.plot(\n",
    "        reduced_features[inds, 0],\n",
    "        reduced_features[inds, 1],\n",
    "        marker=\"o\",\n",
    "        ms=3,\n",
    "        linestyle=\"None\",\n",
    "        alpha=0.5,\n",
    "        label=i)\n",
    "plt.plot(\n",
    "    reduced_cluster_centers[:, 0],\n",
    "    reduced_cluster_centers[:, 1],\n",
    "    marker='o',\n",
    "    ms=8,\n",
    "    linestyle=\"None\",\n",
    "    color=\"black\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0e04d",
   "metadata": {},
   "source": [
    "- The cluster centers are reasonably well separated\n",
    "- Some overlap between them\n",
    "<br>\n",
    "\n",
    "We can now examine samples from within the clusters by:\n",
    "- taking the samples that are closest to their cluster center.\n",
    "- taking the samples that are furthest from (0, 0) for each predicted cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c188ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the closest 10 results to each cluster center\n",
    "closest = {}\n",
    "for i in df_msgs_test[\"label\"].value_counts().index:\n",
    "    inds = df_msgs_test[\"label\"] == i\n",
    "    center = reduced_cluster_centers[i, :]\n",
    "    # use L2 norm, sort by that distance\n",
    "    dists = np.linalg.norm(reduced_features - center, axis=1)\n",
    "    inds_sort = np.argsort(dists)\n",
    "    # take the 10 closest\n",
    "    closest[i] = list(df_msgs_test.iloc[inds_sort].loc[inds, :].index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the messages,\n",
    "for i, inds in closest.items():\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 50)\n",
    "    print(i)\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\n\\n\".join(df_msgs_test.loc[inds, \"msg_strip\"].str[:400]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86d326",
   "metadata": {},
   "source": [
    "From a brief look at the emails, the clearest cluster is 0, which show some kind of automated emails.\n",
    "- 0: Some kind of automated email?\n",
    "- 1, 2: Seem to be the same?\n",
    "<br>\n",
    "\n",
    "Looking at the furthest form (0, 0) for each case, it looks like\n",
    "- 1: to do with scheduling\n",
    "- 2: business emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the furthest 10 results from (0, 0) for each predicted cluster\n",
    "furthest = {}\n",
    "for i in df_msgs_test[\"label\"].value_counts().index:\n",
    "    inds = df_msgs_test[\"label\"] == i\n",
    "    # use L2 norm, sort by that distance\n",
    "    dists = np.linalg.norm(reduced_features, axis=1)\n",
    "    # take the 10 closest\n",
    "    furthest[i] = list(df_msgs_test.iloc[inds_sort].loc[inds, :].index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace15db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the messages\n",
    "for i, inds in furthest.items():\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 50)\n",
    "    print(i)\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\n\\n\".join(df_msgs_test.loc[inds, \"msg_strip\"].str[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93144f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Where the emails came from and went to, by cluster\n",
    "for i in df_msgs_test[\"label\"].value_counts().index:\n",
    "    print(i)\n",
    "    # from\n",
    "    vc_from = (\n",
    "        df_meta_test.loc[df_msgs_test[\"label\"] == i, \"From\"].fillna(\"@no_from\").str.split(\"@\").str[-1].value_counts(normalize=True)\n",
    "    ).to_frame()\n",
    "    # to\n",
    "    vc_to = (\n",
    "        df_meta_test.loc[df_msgs_test[\"label\"] == i, \"To\"].fillna(\"@no_to\").str.split(\"@\").str[-1].value_counts(normalize=True)\n",
    "    ).to_frame()\n",
    "    # Cc\n",
    "    all_cc = []\n",
    "    for sub_list in (\n",
    "        df_meta_test.loc[df_msgs_test[\"label\"] == i, \"Cc\"].fillna(\"@no_cc\").str.split(\", \").apply(lambda x: [i.split(\"@\")[-1] for i in x])\n",
    "    ):\n",
    "        all_cc += sub_list\n",
    "    vc_cc = pd.Series(all_cc).value_counts(normalize=True).to_frame()\n",
    "    vc_cc.columns = [\"Cc\"]\n",
    "    # merge them\n",
    "    vc_from = vc_from.merge(vc_to, left_index=True, right_index=True, how=\"outer\").fillna(0)\n",
    "    vc_from = vc_from.merge(\n",
    "        vc_cc, left_index=True, right_index=True, how=\"outer\"\n",
    "    ).fillna(0).sort_values([\"Cc\", \"From\", \"To\"], ascending=False)\n",
    "    display(vc_from)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cb1ec",
   "metadata": {},
   "source": [
    "Clusters 0 is mostly from Enron email address, clusters 1 and 2 have more external emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280bd1ac",
   "metadata": {},
   "source": [
    "### Topic analysis\n",
    "In addition to the cluster analysis, we carry out a topic analysis. This uses Non-negative Matrix Factorization (NMF), where two non-negative matrices (W, H) are found whose product approximates the input feature matrix X (which is also non-negative, comprising an array of feature vectors for multiple samples).\n",
    "\n",
    "- X: n_samples by n_features\n",
    "- W: n_samples by n_components\n",
    "- H: n_components by n_features\n",
    "\n",
    "<br>\n",
    "\n",
    "The procedure is carried out as follows:\n",
    "- Extract nouns from the email messages. The topic is defined by the nouns in the message. (Note: this is slow, so we only use the val and test datasets.\n",
    "- Fit a TF-IDF vectorizer to the val dataset.\n",
    "- Use this to get feature arrays for the val and test datasets.\n",
    "- Fit the NMF model to the val dataset, for a given number of components (we use 4, but could experiment with more/less: requries a scoring algoirthm to decide the best).\n",
    "- Use this to get an idea of what groups of emails cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d4795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23c478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013001b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning, slow!\n",
    "\n",
    "#df_msgs_train[\"nouns\"] = extract_message_nouns(df_msgs_train[\"msg\"], nlp)\n",
    "#print(\"train done\")\n",
    "df_msgs_val[\"nouns\"] = extract_message_nouns(df_msgs_val[\"msg\"], nlp)\n",
    "print(\"val done\")\n",
    "df_msgs_test[\"nouns\"] = extract_message_nouns(df_msgs_test[\"msg\"], nlp)\n",
    "print(\"test done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ced254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics to extract\n",
    "n_topics = 4\n",
    "# fit TF-IDF vectorizer to the train data\n",
    "# use less features than before, as we only have nouns now\n",
    "vec = TfidfVectorizer(max_features=5000, stop_words=\"english\", max_df=0.95, min_df=2)\n",
    "vec.fit(df_msgs_val[\"nouns\"])\n",
    "\n",
    "# list of unique words found by the vectorizer\n",
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "# extract features for each set\n",
    "#features_train = vec.transform(df_msgs_train[\"nouns\"])\n",
    "features_val = vec.transform(df_msgs_val[\"nouns\"])\n",
    "features_test = vec.transform(df_msgs_test[\"nouns\"])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 25))\n",
    "\n",
    "for j, beta_loss in enumerate([\"kullback-leibler\", \"frobenius\"]):\n",
    "    # git an NMF model the train data\n",
    "    cls = NMF(n_components=n_topics, beta_loss=beta_loss, solver='mu', random_state=0)\n",
    "    cls.fit(features_val)\n",
    "\n",
    "    # number of most influencing words to display per topic\n",
    "    n_top_words = 10\n",
    "    \n",
    "    for i, topic_vec in enumerate(cls.components_):\n",
    "        plt.subplot(5, 2, 2 * i + 1 + j)\n",
    "        inds = topic_vec.argsort()[:-n_top_words - 1:-1]\n",
    "        plt.barh(np.array(feature_names)[inds][::-1], topic_vec[inds][::-1])\n",
    "        plt.title(f\"{i} - {beta_loss}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63ed62",
   "metadata": {},
   "source": [
    "- 0: ECT: Enron Capital and Trade Resources\n",
    "- 1: business (e.g. price, market, company)\n",
    "- 2: communication based (mail, message etc.)\n",
    "- 3: thanking you + scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_labels_test = cls.transform(features_test)\n",
    "nmf_labels_val = cls.transform(features_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many are in each class\n",
    "\n",
    "# Mostly business related class\n",
    "# Least in communication class\n",
    "\n",
    "vc = pd.Series(nmf_labels_test.argmax(axis=1)).value_counts().to_frame().reset_index().sort_values(\"index\")\n",
    "vc.columns = [\"cluster\", \"count\"]\n",
    "ind_map = {\n",
    "    0: \"ECT\",\n",
    "    1: \"business\",\n",
    "    2: \"communication\",\n",
    "    3: \"scheduling\"\n",
    "}\n",
    "\n",
    "vc[\"label\"] = vc[\"cluster\"].map(ind_map)\n",
    "display(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad19d89",
   "metadata": {},
   "source": [
    "### Conclusions from the data\n",
    "\n",
    "- The dataset can be used to provide insights into employees email habits (e.g. rates, size of emails sent, when they are sent etc.). This assumes that the dataset is a representative sample of all emails sent by these users.\n",
    "- The dataset requires cleaning before use (parsing the emails into metadata and body, dealing with missing/incorrect metadata etc.)\n",
    "- An initial analysis of the messages shows clusters of topics, e.g. relating to scheduling meetings or to business information.\n",
    "\n",
    "##### Further work\n",
    "- The main improvement to this analysis would involve improved parsing of the email message bodies.\n",
    "- The clustering used here could be used as the starting point to labelling the data:\n",
    "    - label work and personal emails, to classify work emails for further analysis\n",
    "    - label routine emails (e.g. automatic meeting reminders) and non-routine, so that routine emails can be removed from the dataset\n",
    "- Reconstructing the email chain (i.e. find replies to emails) could be used to give insights into e.g. what types of messages elicit responses, what subjects are best etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68250bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
